{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d70059e-096d-4a95-9e2d-b9ed0b0fe71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.3\n",
      "0.6.0\n",
      "4.3.3\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import konlpy\n",
    "import gensim\n",
    "\n",
    "print(pandas.__version__)\n",
    "print(konlpy.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6abac8e0-212c-463e-b28b-0f21f1b3e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"C:/Users/양자/Desktop/Hun_Works/AIFFEL_quest_rs/Exploration/ex05/ratings_train.txt\"\n",
    "test_data_path = \"C:/Users/양자/Desktop/Hun_Works/AIFFEL_quest_rs/Exploration/ex05/ratings_test.txt\"\n",
    "train_data = open(train_data_path, 'r', encoding = 'utf-8')\n",
    "test_data = open(test_data_path, 'r', encoding = 'utf-8')\n",
    "\n",
    "train_data_list = train_data.readlines()\n",
    "test_data_list = test_data.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b83df74-1bbf-45fd-a2dd-1fad535c8aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id\\tdocument\\tlabel\\n',\n",
       " '9976970\\t아 더빙.. 진짜 짜증나네요 목소리\\t0\\n',\n",
       " '3819312\\t흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\\t1\\n',\n",
       " '10265843\\t너무재밓었다그래서보는것을추천한다\\t0\\n',\n",
       " '9045019\\t교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\\t0\\n',\n",
       " '6483659\\t사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\\t1\\n',\n",
       " '5403919\\t막 걸음마 뗀 3세부터 초등학교 1학년생인 8살용영화.ㅋㅋㅋ...별반개도 아까움.\\t0\\n',\n",
       " '7797314\\t원작의 긴장감을 제대로 살려내지못했다.\\t0\\n',\n",
       " '9443947\\t별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네\\t0\\n',\n",
       " '7156791\\t액션이 없는데도 재미 있는 몇안되는 영화\\t1\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a427841d-685f-4dc9-84e5-3d566c0a21cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨이랑 텍스트랑 나누기\n",
    "def parse_data(data_lines):\n",
    "    parsed_data = []\n",
    "    for line in data_lines[1:]:\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) < 3:\n",
    "            continue  # 데이터 형식에 문제가 있는 경우 건너뜁니다.\n",
    "        # parts[0]: id, parts[1]: document, parts[2]: label\n",
    "        document = parts[1]\n",
    "        try:\n",
    "            label = int(parts[2])\n",
    "        except ValueError:\n",
    "            continue  # label 변환에 실패하면 해당 줄 건너뛰기\n",
    "        parsed_data.append((document, label))\n",
    "    return parsed_data\n",
    "    \n",
    "parsed_train_data = parse_data(train_data_list)\n",
    "parsed_test_data = parse_data(test_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2431d448-221c-4104-b560-321b81b3e013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('아 더빙.. 진짜 짜증나네요 목소리', 0), ('흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나', 1), ('너무재밓었다그래서보는것을추천한다', 0)]\n"
     ]
    }
   ],
   "source": [
    "print(parsed_train_data[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9f0243a1-be10-44ca-a414-c6d76f78a4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#불용어 제거\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    tokens = okt.morphs(sentence)  # 형태소 분석\n",
    "    return \" \".join([word for word in tokens if word not in stopwords])  # 불용어 제거 후 다시 문자열로 변환\n",
    "\n",
    "# 문서와 라벨 분리\n",
    "train_texts = [doc for doc, label in parsed_train_data]\n",
    "train_labels = [label for doc, label in parsed_train_data]\n",
    "test_texts  = [doc for doc, label in parsed_test_data]\n",
    "test_labels  = [label for doc, label in parsed_test_data]\n",
    "\n",
    "# 불용어 제거 (문서 전처리)\n",
    "cleaned_train_texts = [remove_stopwords(text) for text in train_texts]\n",
    "cleaned_test_texts  = [remove_stopwords(text) for text in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5338c4f1-9884-432a-b76b-45a784881d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "896ae7b0-7bd9-4a37-ba44-7bee95445c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아 더빙 .. 진짜 짜증나네요 목소리', '흠 ... 포스터 보고 초딩 영화 줄 .... 오버 연기 조차 가볍지 않구나', '너 무재 밓었 다그 래서 보는것을 추천 다']\n",
      "['굳 ㅋ', 'GDNTOPCLASSINTHECLUB', '뭐 야 평점 .... 나쁘진 않지만 10 점 짜 리 더 더욱 아니잖아']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_train_texts[:3])\n",
    "print(cleaned_test_texts[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01b3b507-9c3a-4d82-b767-01ef7df337b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_texts, X_val_texts, y_train, y_val = train_test_split(cleaned_train_texts,\n",
    "                                                              train_labels,\n",
    "                                                              test_size=0.2,\n",
    "                                                              random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb5a6812-daa9-419b-a0cd-583675710727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 빈도 세는 함수 -> 상위1만개 짜를거\n",
    "def build_counter(sentences):\n",
    "    counter = collections.Counter()\n",
    "    for sentence in sentences:\n",
    "        tokens = okt.morphs(sentence)\n",
    "        counter.update(tokens)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3999593f-74ec-4c92-a479-f650503807df",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_counter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_texts\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 3\u001b[0m, in \u001b[0;36mbuild_counter\u001b[1;34m(sentences)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_counter\u001b[39m(sentences):\n\u001b[1;32m----> 3\u001b[0m     counter \u001b[38;5;241m=\u001b[39m \u001b[43mcollections\u001b[49m\u001b[38;5;241m.\u001b[39mCounter()\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m      5\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m okt\u001b[38;5;241m.\u001b[39mmorphs(sentence)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "counter = build_counter(X_train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef619f7-5484-4d41-924c-b30a22de53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 4개를 시작할 때 넣고, 이후부터 상위 1만개로 컷\n",
    "def build_word_to_index(counter, top_n=10000, special_tokens=[\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]):\n",
    "    word_to_index = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "    index = len(special_tokens)\n",
    "    most_common = counter.most_common(top_n)\n",
    "    for word, _ in most_common:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = index\n",
    "            index += 1\n",
    "    return word_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36270f49-6bea-42b5-b741-37e231390f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = build_word_to_index(counter, top_n=10000)\n",
    "index_to_word = {index: word for word, index in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578c4472-19db-4f72-b92d-a7028ca4ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_sentence(sentence, word_to_index):\n",
    "    # <BOS> 시작\n",
    "    return [word_to_index['<BOS>']] + [\n",
    "        word_to_index[word] if word in word_to_index else word_to_index['<UNK>']\n",
    "        for word in sentence.split()\n",
    "    ]\n",
    "\n",
    "def get_encoded_sentences(sentences, word_to_index):\n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word):\n",
    "    # <BOS> 토큰은 제외 디코딩\n",
    "    return ' '.join(\n",
    "        index_to_word[index] if index in index_to_word else '<UNK>'\n",
    "        for index in encoded_sentence[1:]\n",
    "    )\n",
    "\n",
    "def get_decoded_sentences(encoded_sentences, index_to_word):\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bdca94-3276-4e93-8ae3-8e2c92436b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#테스트 동작 확인ㅌ\n",
    "test_sentence = '안녕 야말'\n",
    "test_sentences = [test_sentence, \"사랑 싫어\"]\n",
    "\n",
    "encoded_list = get_encoded_sentences(test_sentences, word_to_index)\n",
    "\n",
    "decoded_list = get_decoded_sentences(encoded_list, index_to_word)\n",
    "\n",
    "print(encoded_list)\n",
    "print(decoded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5055dfe9-3f65-4e12-977e-17c3cbe363b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습, 검증, 테스트 인코딩\n",
    "encoded_train = get_encoded_sentences(X_train_texts, word_to_index)\n",
    "encoded_val   = get_encoded_sentences(X_val_texts, word_to_index)\n",
    "encoded_test  = get_encoded_sentences(cleaned_test_texts, word_to_index)\n",
    "\n",
    "#최대 길이를 기준패딩\n",
    "max_length = max(len(seq) for seq in encoded_train)\n",
    "padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')\n",
    "padded_val   = pad_sequences(encoded_val, maxlen=max_length, padding='post')\n",
    "padded_test  = pad_sequences(encoded_test, maxlen=max_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3c2970-3118-483f-af81-f35e73e08b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cf192b-415a-43e4-bf7d-3186f9ef4b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 결과 시각화\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 손실 시각화\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 정확도 시각화\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63ddcb6-b4c5-4d65-b45f-327a3c271a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, word_vector_dim, input_shape=(None,), name=\"embedding\"),\n",
    "    layers.LSTM(256),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=3),\n",
    "            keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "                                            monitor=\"val_loss\",\n",
    "                                            save_best_only=True)]\n",
    "\n",
    "history = model.fit(padded_train, np.array(y_train), epochs=20, callbacks = callbacks, validation_data=(padded_val, np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d030f5-e6fb-4984-be82-1c648599edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b181484-1ca4-416b-987b-53c5877d10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946d428c-1f19-4e5a-8330-b8f943b8ddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = model.predict(padded_test)\n",
    "y_pred = (pred_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# cm 구하기\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ae7cf1-bc8d-4327-998e-f35b1cb2033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model_conv = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, word_vector_dim, input_length=max_length, name=\"embedding_1dconv\"),\n",
    "    layers.Conv1D(filters=16, kernel_size=5, activation='relu',\n",
    "                  kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_conv.summary()\n",
    "\n",
    "model_conv.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=3),\n",
    "            keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "                                            monitor=\"val_loss\",\n",
    "                                            save_best_only=True)]\n",
    "\n",
    "history_conv = model_conv.fit(padded_train, np.array(y_train),\n",
    "                              epochs=10,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=(padded_val, np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436eb245-a086-4d00-8d89-d0188a37b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7ab50d-e54a-4a90-8e8f-7f4b8a2d3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = model_conv.predict(padded_test)\n",
    "y_pred = (pred_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# cm 구하기\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2d672-2426-4cbd-9b81-61cec2d73fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi_lstm = keras.Sequential([\n",
    "    layers.Embedding(vocab_size, word_vector_dim, input_length=max_length, name=\"embedding_bi_lstm\"),\n",
    "    layers.Bidirectional(layers.LSTM(128)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "callbacks = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=3)\n",
    "\n",
    "model_bi_lstm.compile(optimizer='adam',\n",
    "                      loss='binary_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "model_bi_lstm.summary()\n",
    "\n",
    "model_bi_lstm.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=3),\n",
    "            keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "                                            monitor=\"val_loss\",\n",
    "                                            save_best_only=True)]\n",
    "\n",
    "history_bi_lstm = model_bi_lstm.fit(padded_train, np.array(y_train),\n",
    "                              epochs=10,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=(padded_val, np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac35e0dc-b661-42f6-ad96-b42780316661",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_bi_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd787c-b9c6-4053-8021-2ba1ca250b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = model_bi_lstm.predict(padded_test)\n",
    "y_pred = (pred_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# cm 구하기\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e388e2-577a-4c83-a4f2-1556de7af74f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import Word2VecKeyedVectors\n",
    "\n",
    "word2vec_file_path = os.path.abspath(\"C:/temp files/word2vec_ko.model\")\n",
    "\n",
    "word_vectors = Word2VecKeyedVectors.load(word2vec_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f049e4-7f4d-434b-8b69-a6fba10ab50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100  # Word2Vec 모델의 임베딩 차원\n",
    "vocab_size = len(word_to_index)  # 예를 들어, 상위 10,000 단어 + 4 특수 토큰 등\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_to_index.items():\n",
    "    if word in word_vectors.wv:\n",
    "        embedding_matrix[i] = word_vectors.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e67035-3eca-423b-a539-0b1da6f44069",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=vocab_size,\n",
    "                                   output_dim=embedding_dim,\n",
    "                                   weights=[embedding_matrix],\n",
    "                                   input_length=max_length,\n",
    "                                   trainable=False,\n",
    "                                   name=\"embedding_layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54524061-8f8a-4d1f-9e15-5343d42b2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model_conv = keras.Sequential([\n",
    "    embedding_layer,\n",
    "    layers.Conv1D(filters=16, kernel_size=5, activation='relu',\n",
    "                  padding='same',\n",
    "                  kernel_regularizer=regularizers.l2(0.0005)),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.0005)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_conv.summary()\n",
    "\n",
    "model_conv.compile(optimizer='adam',\n",
    "                   loss='binary_crossentropy',\n",
    "                   metrics=['accuracy'])\n",
    "\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         patience=3),\n",
    "            keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5',\n",
    "                                            monitor=\"val_loss\",\n",
    "                                            save_best_only=True)]\n",
    "\n",
    "history_conv = model_conv.fit(padded_train, np.array(y_train),\n",
    "                              epochs=10,\n",
    "                              callbacks=callbacks,\n",
    "                              validation_data=(padded_val, np.array(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7d0ad7-5382-4102-85df-dbd356a1c2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_conv)\n",
    "pred_probs = model_conv.predict(padded_test)\n",
    "y_pred = (pred_probs > 0.5).astype(int).reshape(-1)\n",
    "\n",
    "# cm 구하기\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f8ebc-57c4-496a-97d2-2189f3f71f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    오늘의 회고:\n",
    "    0. 오후 5시 19분이 되어서야 예시답안이 있음을 깨달았다 . . . \n",
    "    1. 너무 시간이 없었음\n",
    "    2. 하이퍼파라미터랑 모델 튜닝을 계속 시도했는데 어떻게 이럴 수 있나 싶을정도로 85퍼가 안나옴\n",
    "        1D conv 레이어가 그나마 84퍼가 뜨는데, word2vec 쓰니까 오히려 효율이 떨어짐. 내 모델엔 안맞나...\n",
    "    3. lstm을 쓸 때, confusion matrix를 보면 알겠지만 true 로 예측한 것ㅇ\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348b934f-4100-4deb-81ac-a569ee6eefcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d17ba-a580-44e9-9f98-d68ea5c275c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24416f61-5254-4d99-a8d0-e05326c8a11e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
